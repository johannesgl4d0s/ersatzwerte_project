{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# SETTINGS\n",
    "# =========================\n",
    "BASE_DIR    = Path(\"../data\")\n",
    "MODELS_DIR  = BASE_DIR / \"sarimax_models\"\n",
    "ASSIGN_FILE = BASE_DIR / \"assignments\" / \"assignment_auto.csv\"\n",
    "OUT_DIR     = BASE_DIR / \"experiment_results\"\n",
    "IMPUTE_DIR  = OUT_DIR / \"imputed_series\"   # <— hier landen die Parquet-Dateien\n",
    "FREQ        = \"h\"\n",
    "TARGET_COL  = \"consumption\"\n",
    "GAP_WINDOWS = {\"short6h\": (3, 6), \"day24h\": (2, 24), \"long72h\": (1, 72)}\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    \"rmse_toleranz\": ['hour', 'w_tl', 'w_ff', 'w_tb10', 'w_tb20', 'CEGH_WAP', 'THE_WAP'],\n",
    "    \"bester_score\":  ['w_tl','w_tb10'],\n",
    "    \"minimaler_rmse\":['hour','weekday','month','is_weekend','w_tl','w_rf','w_ff','w_ffx',\n",
    "                      'w_cglo','w_so_h','w_rr','w_tb10','w_tb20','CEGH_WAP','THE_WAP']\n",
    "}\n",
    "TIME_FEATS = {\"hour\",\"weekday\",\"month\",\"is_weekend\"}\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def ensure_outdir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_assignments_csv(p: Path) -> pd.DataFrame:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Assignment-Datei fehlt: {p}\")\n",
    "    df = pd.read_csv(p)\n",
    "    required = {\"series_key\",\"assigned_medoid\",\"bin\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        miss = required - set(df.columns)\n",
    "        raise KeyError(f\"Assignment-Spalten fehlen: {miss}\")\n",
    "    return df\n",
    "\n",
    "def model_path_for(medoid: str, variant: str) -> Path:\n",
    "    return MODELS_DIR / f\"{medoid}__{variant}.pkl\"\n",
    "\n",
    "def load_model_safe(medoid: str, variant: str):\n",
    "    p = model_path_for(medoid, variant)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Modell fehlt: {p.name}\")\n",
    "    with open(p, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def select_exog(df: pd.DataFrame, features: List[str]) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    missing = [c for c in features if c not in df.columns]\n",
    "    return df[[c for c in features if c in df.columns]], missing\n",
    "\n",
    "def metrics(y_true: pd.Series, y_pred: pd.Series) -> Dict[str, float]:\n",
    "    mask = y_true.notna() & y_pred.notna()\n",
    "    if mask.sum() == 0:\n",
    "        return {\"rmse\": np.nan, \"mae\": np.nan, \"mape\": np.nan}\n",
    "    e = (y_true[mask] - y_pred[mask])\n",
    "    rmse = float(np.sqrt(np.mean(np.square(e))))\n",
    "    mae  = float(np.mean(np.abs(e)))\n",
    "    denom = y_true[mask].replace(0, np.nan)\n",
    "    mape = float(np.mean(np.abs(e / denom))) * 100.0\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"mape\": mape}\n",
    "\n",
    "def random_gap_mask(idx: pd.DatetimeIndex, n_windows: int, win_len_hours: int, rng: np.random.Generator) -> pd.Series:\n",
    "    mask = pd.Series(False, index=idx)\n",
    "    if len(idx) < win_len_hours:\n",
    "        return mask\n",
    "    positions = np.arange(0, len(idx) - win_len_hours + 1)\n",
    "    rng.shuffle(positions)\n",
    "    starts = []\n",
    "    for pos in positions:\n",
    "        if all(abs(pos - s) >= win_len_hours for s in starts):\n",
    "            starts.append(pos)\n",
    "            if len(starts) >= n_windows:\n",
    "                break\n",
    "    for s in starts:\n",
    "        sel = idx[s : s + win_len_hours]\n",
    "        mask.loc[sel] = True\n",
    "    return mask\n",
    "\n",
    "def inject_gaps(series_df: pd.DataFrame, patterns: Dict[str, Tuple[int, int]], rng: np.random.Generator) -> Dict[str, pd.Series]:\n",
    "    idx = series_df.index\n",
    "    return {name: random_gap_mask(idx, n, L, rng) for name, (n, L) in patterns.items()}\n",
    "\n",
    "def get_model_index(res) -> pd.DatetimeIndex:\n",
    "    # Versuch 1: direkt aus dem Modell\n",
    "    idx = getattr(res.model, \"_index\", None)\n",
    "    if isinstance(idx, pd.DatetimeIndex):\n",
    "        return idx\n",
    "    # Versuch 2: aus res.data.dates rekonstruieren\n",
    "    if getattr(res, \"data\", None) is not None and getattr(res.data, \"dates\", None) is not None:\n",
    "        start = pd.to_datetime(res.data.dates[0])\n",
    "        try:\n",
    "            freq = getattr(res.model.data.row_labels, \"freqstr\", None) or \"H\"\n",
    "        except Exception:\n",
    "            freq = \"H\"\n",
    "        return pd.date_range(start=start, periods=int(res.nobs), freq=freq)\n",
    "    raise ValueError(\"Konnte den Trainingsindex des Modells nicht ermitteln.\")\n",
    "\n",
    "\n",
    "def impute_with_applied_model(res,\n",
    "                              y_nan: pd.Series,\n",
    "                              exog_full: pd.DataFrame,\n",
    "                              do_calibrate: bool = True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Wendet die bereits gefitteten SARIMAX-Parameter auf *die gesamte Serie* an.\n",
    "    Dadurch stimmen Länge/Index (keine 8760-vs-35058-Probleme).\n",
    "    Optional: Level-Kalibrierung a+b*hat, um Regime-/Skalenwechsel abzufangen.\n",
    "    \"\"\"\n",
    "    # Exognamen aus dem Trainingsmodell holen & exakt reordnen\n",
    "    trained_names = getattr(getattr(res, \"model\", None), \"exog_names\", None)\n",
    "    if trained_names is not None:\n",
    "        exog_full = align_exogs_for_model(exog_full, trained_names)\n",
    "\n",
    "    # Model auf neuen Datensatz \"anwenden\" (keine Neuschätzung der Parameter)\n",
    "    res_applied = res.apply(endog=y_nan, exog=exog_full, refit=False)\n",
    "\n",
    "    # Vorhersage über den kompletten Index\n",
    "    pred = res_applied.get_prediction(start=y_nan.index[0],\n",
    "                                      end=y_nan.index[-1]).predicted_mean\n",
    "\n",
    "    # Level-Kalibrierung (kurzes Fenster)\n",
    "    if do_calibrate:\n",
    "        a, b = level_calibration(y_true=y_nan, y_pred=pred)\n",
    "        pred = a + b*pred\n",
    "\n",
    "    # Nur dort füllen, wo NaN war\n",
    "    out = y_nan.copy()\n",
    "    needs = out.isna()\n",
    "    out.loc[needs] = pred.loc[needs]\n",
    "    return out\n",
    "\n",
    "# def impute_with_sarimax_aligned(res, endog: pd.Series, exog: pd.DataFrame) -> pd.Series:\n",
    "#     # 1) Auf Trainingsindex ausrichten\n",
    "#     model_idx = get_model_index(res)\n",
    "#     y_aligned = endog.reindex(model_idx)\n",
    "#     x_aligned = exog.reindex(model_idx)\n",
    "#     if x_aligned.isna().any().any():\n",
    "#         x_aligned = x_aligned.ffill().bfill()\n",
    "#     # 2) Vorhersage auf exakt dieser Spanne\n",
    "#     pred = res.get_prediction(start=model_idx[0], end=model_idx[-1], exog=x_aligned).predicted_mean\n",
    "#     # 3) Zurück auf Originalindex und nur NaNs füllen\n",
    "#     out = endog.copy()\n",
    "#     needs = out.isna()\n",
    "#     common = out.index.intersection(model_idx)\n",
    "#     fill_mask = needs.loc[common]\n",
    "#     out.loc[fill_mask.index] = pred.loc[fill_mask.index]\n",
    "#\n",
    "#     return out\n",
    "\n",
    "def prepare_series_for_imputation(\n",
    "    df_raw: pd.DataFrame,\n",
    "    target_col: str = \"consumption\",\n",
    "    exog_cols: Optional[List[str]] = None,\n",
    "    freq: str = \"h\",\n",
    ") -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    if getattr(idx, \"tz\", None) is not None:\n",
    "        idx = idx.tz_convert(\"UTC\").tz_localize(None)\n",
    "    df.index = idx\n",
    "    df = df[~df.index.duplicated(keep=\"first\")].sort_index()\n",
    "    df = df.asfreq(freq)\n",
    "\n",
    "    # Zeitfeatures sicherstellen\n",
    "    if \"hour\" not in df.columns: df[\"hour\"] = df.index.hour\n",
    "    if \"weekday\" not in df.columns: df[\"weekday\"] = df.index.weekday\n",
    "    if \"month\" not in df.columns: df[\"month\"] = df.index.month\n",
    "    if \"is_weekend\" not in df.columns:\n",
    "        df[\"is_weekend\"] = (df.index.weekday >= 5).astype(int)\n",
    "    else:\n",
    "        df[\"is_weekend\"] = df[\"is_weekend\"].astype(int, errors=\"ignore\")\n",
    "\n",
    "    # Exogene (alles außer Target und Zeitfeatures), sanft auffüllen\n",
    "    if exog_cols is None:\n",
    "        exog_cols = [c for c in df.columns if c not in {target_col, *TIME_FEATS}]\n",
    "    exog_cols_existing = [c for c in exog_cols if c in df.columns]\n",
    "    if exog_cols_existing:\n",
    "        df[exog_cols_existing] = (\n",
    "            df[exog_cols_existing].ffill().bfill()\n",
    "                                   .interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        )\n",
    "    return df\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "INVALID_WIN_CHARS = r'[<>:\"/\\\\|?*]'  # verbotene Zeichen auf Windows\n",
    "\n",
    "def sanitize_for_path(s: str) -> str:\n",
    "    # Unicode normalisieren und trimmen\n",
    "    s = unicodedata.normalize(\"NFKC\", s).strip()\n",
    "    # Sonderfall: führendes \">\" abfangen\n",
    "    if s.startswith(\">\"):\n",
    "        s = \"gt\" + s[1:]   # z.B. \">30%\" -> \"gt30%\"\n",
    "    # Verbotene Zeichen ersetzen\n",
    "    s = re.sub(INVALID_WIN_CHARS, \"_\", s)\n",
    "    # Keine Leerzeichen/Punkte am Ende\n",
    "    s = s.rstrip(\" .\")\n",
    "    return s or \"unnamed\"\n",
    "\n",
    "# =========================\n",
    "# MAIN EXPERIMENT RUNNER\n",
    "# =========================\n",
    "# =========================\n",
    "SAVE_SERIES_PARQUET = True\n",
    "SAVE_SERIES_PNG     = False  # auf True setzen, wenn du Plots pro Fall als PNG willst\n",
    "IMPUTED_DIR         = OUT_DIR / \"imputed_series\"\n",
    "\n",
    "def _safe(s: str) -> str:\n",
    "    \"\"\"Datei-/Ordner-sichere Namen (ersetzt problematische Zeichen).\"\"\"\n",
    "    return (\n",
    "        str(s)\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\"\\\\\", \"_\")\n",
    "        .replace(\":\", \"_\")\n",
    "        .replace(\"*\", \"_\")\n",
    "        .replace(\"?\", \"_\")\n",
    "        .replace('\"', \"_\")\n",
    "        .replace(\"<\", \"_\")\n",
    "        .replace(\">\", \"_\")\n",
    "        .replace(\"|\", \"_\")\n",
    "    )\n",
    "\n",
    "def run_experiments_with_diagnostics(series_by_bin_loaded: Dict[str, Dict[str, pd.DataFrame]]):\n",
    "    print(\"[PATH] CWD:      \", Path.cwd())\n",
    "    print(\"[PATH] BASE_DIR: \", BASE_DIR.resolve())\n",
    "    print(\"[PATH] OUT_DIR:  \", OUT_DIR.resolve())\n",
    "    print(\"[PATH] ASSIGN:   \", ASSIGN_FILE.resolve(), \"exists:\", ASSIGN_FILE.exists())\n",
    "\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    ensure_outdir(OUT_DIR)\n",
    "    ensure_outdir(IMPUTED_DIR)\n",
    "\n",
    "    # 1) Assignments\n",
    "    try:\n",
    "        assignments = load_assignments_csv(ASSIGN_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] {e}\")\n",
    "        return\n",
    "\n",
    "    assign_lookup = assignments.set_index(\"series_key\")[\"assigned_medoid\"].to_dict()\n",
    "    bin_lookup    = assignments.set_index(\"series_key\")[\"bin\"].to_dict()\n",
    "\n",
    "    variants = list(FEATURE_SETS.keys())\n",
    "    print(\"[INFO] Varianten:\", variants)\n",
    "\n",
    "    results_rows, skip_rows = [], []\n",
    "    first_success_case = None\n",
    "\n",
    "    for bin_key, series_map in series_by_bin_loaded.items():\n",
    "        for series_key, df_raw in series_map.items():\n",
    "\n",
    "            # 0) Target da?\n",
    "            if TARGET_COL not in df_raw.columns:\n",
    "                skip_rows.append({\n",
    "                    \"series_key\": series_key, \"bin\": bin_key, \"variant\": None, \"gap_pattern\": None,\n",
    "                    \"stage\": \"target_check\", \"reason\": f\"Target '{TARGET_COL}' fehlt\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # 1) Prep (Target NICHT auffüllen)\n",
    "            try:\n",
    "                df = prepare_series_for_imputation(\n",
    "                    df_raw=df_raw, target_col=TARGET_COL, exog_cols=None, freq=FREQ\n",
    "                )\n",
    "            except Exception as e:\n",
    "                skip_rows.append({\n",
    "                    \"series_key\": series_key, \"bin\": bin_key, \"variant\": None, \"gap_pattern\": None,\n",
    "                    \"stage\": \"prep\", \"reason\": str(e)\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # 2) Medoid\n",
    "            medoid = assign_lookup.get(series_key)\n",
    "            if medoid is None:\n",
    "                skip_rows.append({\n",
    "                    \"series_key\": series_key, \"bin\": bin_key, \"variant\": None, \"gap_pattern\": None,\n",
    "                    \"stage\": \"assignment\", \"reason\": \"kein assigned_medoid\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # 3) synthetische Lücken\n",
    "            gap_masks = inject_gaps(df, GAP_WINDOWS, rng=rng)\n",
    "\n",
    "            # 4) Varianten durchgehen\n",
    "            for variant in variants:\n",
    "                feats = FEATURE_SETS[variant]\n",
    "\n",
    "                # 4a) Modell laden\n",
    "                try:\n",
    "                    res = load_model_safe(medoid, variant)\n",
    "                except Exception as e:\n",
    "                    skip_rows.append({\n",
    "                        \"series_key\": series_key, \"bin\": bin_key, \"variant\": variant, \"gap_pattern\": None,\n",
    "                        \"stage\": \"model_load\", \"reason\": str(e)\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # 4b) Exogene gemäß Variante\n",
    "                exog_full, missing = select_exog(df, feats)\n",
    "                if missing:\n",
    "                    skip_rows.append({\n",
    "                        \"series_key\": series_key, \"bin\": bin_key, \"variant\": variant, \"gap_pattern\": None,\n",
    "                        \"stage\": \"exog_missing\", \"reason\": f\"fehlen: {missing}\"\n",
    "                    })\n",
    "                    if exog_full.shape[1] == 0:\n",
    "                        continue\n",
    "\n",
    "                # Reihenfolge/Set der Exogs mit Trainings-Modell abgleichen\n",
    "                trained_names = getattr(getattr(res, \"model\", None), \"exog_names\", None)\n",
    "                if trained_names is not None:\n",
    "                    miss = [c for c in trained_names if c not in exog_full.columns]\n",
    "                    extra= [c for c in exog_full.columns if c not in trained_names]\n",
    "                    if miss or extra:\n",
    "                        skip_rows.append({\n",
    "                            \"series_key\": series_key, \"bin\": bin_key, \"variant\": variant, \"gap_pattern\": None,\n",
    "                            \"stage\": \"exog_mismatch\", \"reason\": f\"missing:{miss} extra:{extra}\"\n",
    "                        })\n",
    "                        continue\n",
    "                    exog_full = exog_full[trained_names]\n",
    "\n",
    "                # 4c) je Gap-Pattern\n",
    "                for gap_name, mask in gap_masks.items():\n",
    "                    y_true = df[TARGET_COL].copy()\n",
    "                    y_nan  = y_true.copy()\n",
    "                    if mask.sum() == 0:\n",
    "                        skip_rows.append({\n",
    "                            \"series_key\": series_key, \"bin\": bin_key, \"variant\": variant, \"gap_pattern\": gap_name,\n",
    "                            \"stage\": \"gap_empty\", \"reason\": \"kein synthetischer Gap möglich\"\n",
    "                        })\n",
    "                        continue\n",
    "                    # synthetische NaNs setzen\n",
    "                    y_nan.loc[mask[mask].index] = np.nan\n",
    "\n",
    "                    # 4d) Imputation (WICHTIG: aligned apply)\n",
    "                    try:\n",
    "                        y_imp = impute_with_sarimax_aligned(res, y_nan, exog_full)\n",
    "                    except Exception as e:\n",
    "                        skip_rows.append({\n",
    "                            \"series_key\": series_key, \"bin\": bin_key, \"variant\": variant, \"gap_pattern\": gap_name,\n",
    "                            \"stage\": \"predict\", \"reason\": str(e)\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    # 4e) Metriken nur im Gap\n",
    "                    eval_mask = mask & y_true.notna()\n",
    "                    m = metrics(y_true[eval_mask], y_imp[eval_mask])\n",
    "\n",
    "                    results_rows.append({\n",
    "                        \"bin\": bin_key,\n",
    "                        \"series_key\": series_key,\n",
    "                        \"assignment_bin\": bin_lookup.get(series_key, np.nan),\n",
    "                        \"assigned_medoid\": medoid,\n",
    "                        \"variant\": variant,\n",
    "                        \"features\": \",\".join(feats),\n",
    "                        \"gap_pattern\": gap_name,\n",
    "                        \"n_eval_points\": int(eval_mask.sum()),\n",
    "                        **m\n",
    "                    })\n",
    "\n",
    "                    # 4f) (neu) Diagnose-Datei speichern\n",
    "                    if SAVE_SERIES_PARQUET or SAVE_SERIES_PNG:\n",
    "                        var_dir = IMPUTED_DIR / _safe(variant) / _safe(bin_key)\n",
    "                        ensure_outdir(var_dir)\n",
    "                        base_fn = f\"{_safe(series_key)}__{_safe(gap_name)}\"\n",
    "                        if SAVE_SERIES_PARQUET:\n",
    "                            pd.DataFrame({\n",
    "                                \"y_true\": y_true,\n",
    "                                \"y_with_nan\": y_nan,\n",
    "                                \"y_imputed\": y_imp,\n",
    "                                \"gap_mask\": mask.astype(int)\n",
    "                            }).to_parquet(var_dir / f\"{base_fn}.parquet\")\n",
    "                        if SAVE_SERIES_PNG:\n",
    "                            import matplotlib.pyplot as plt\n",
    "                            plt.figure(figsize=(14,4))\n",
    "                            plt.plot(y_true.index, y_true.values, label=\"Original\", linewidth=1)\n",
    "                            plt.plot(y_imp.index,  y_imp.values,  label=\"Imputation\", linewidth=1, alpha=0.8)\n",
    "                            plt.fill_between(mask.index, y_true.min(), y_true.max(),\n",
    "                                             where=mask.values, alpha=0.15, label=\"synthetische Lücke\")\n",
    "                            plt.legend(); plt.tight_layout()\n",
    "                            plt.title(f\"{series_key} – {variant} – {gap_name}\")\n",
    "                            plt.savefig(var_dir / f\"{base_fn}.png\", dpi=120)\n",
    "                            plt.close()\n",
    "\n",
    "                    if first_success_case is None:\n",
    "                        first_success_case = (series_key, bin_key, variant, gap_name)\n",
    "\n",
    "    # 5) Logs schreiben\n",
    "    ensure_outdir(OUT_DIR)\n",
    "    debug_df = pd.DataFrame(skip_rows)\n",
    "    res_df   = pd.DataFrame(results_rows)\n",
    "\n",
    "    debug_path = OUT_DIR / \"debug_skips.csv\"\n",
    "    res_path   = OUT_DIR / \"experiment_summary.csv\"\n",
    "\n",
    "    debug_df.to_csv(debug_path, index=False)\n",
    "    print(f\"[OK] Skip-Log: {debug_path.resolve()} (rows={len(debug_df)})\")\n",
    "\n",
    "    if not res_df.empty:\n",
    "        res_df.to_csv(res_path, index=False)\n",
    "        print(f\"[OK] Ergebnisse: {res_path.resolve()} (rows={len(res_df)})\")\n",
    "    else:\n",
    "        print(\"[WARN] Keine Ergebnisse erzeugt.\")\n",
    "\n",
    "    if first_success_case:\n",
    "        sk, bk, var, gp = first_success_case\n",
    "        print(f\"[SMOKE] Erste erfolgreiche Kombination: {sk} | {bk} | {var} | {gp}\")\n",
    "    else:\n",
    "        print(\"[SMOKE] Keine erfolgreiche Kombination gefunden — bitte Skip-Log ansehen.\")\n",
    "\n",
    "# =========================\n",
    "# PLOT-FUNKTION\n",
    "# =========================\n",
    "def plot_imputation(series_key: str,\n",
    "                    variant: str,\n",
    "                    bin_key: str,\n",
    "                    gap_pattern: str,\n",
    "                    base_dir: Path = IMPUTE_DIR,\n",
    "                    target_col: str = TARGET_COL):\n",
    "    safe_bin = sanitize_for_path(bin_key)\n",
    "    fn = Path(base_dir) / variant / safe_bin / f\"{series_key}__{gap_pattern}.parquet\"\n",
    "    if not fn.exists():\n",
    "        raise FileNotFoundError(f\"Datei nicht gefunden: {fn}\")\n",
    "    df = pd.read_parquet(fn)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(df.index, df[target_col], label=\"Original\", alpha=0.7, lw=2)\n",
    "    plt.plot(df.index, df[f\"{target_col}_imputed\"], label=\"Imputation\", alpha=0.7, lw=2)\n",
    "\n",
    "    mask = df[\"gap_mask\"].astype(bool)\n",
    "    if mask.any():\n",
    "        ymin = np.nanmin(df[[target_col, f\"{target_col}_imputed\"]].values)\n",
    "        ymax = np.nanmax(df[[target_col, f\"{target_col}_imputed\"]].values)\n",
    "        plt.fill_between(df.index, ymin, ymax, where=mask, alpha=0.15, label=\"synthetische Lücke\")\n",
    "\n",
    "    plt.title(f\"{series_key} – {variant} – {gap_pattern}\")\n",
    "    plt.xlabel(\"Zeit\")\n",
    "    plt.ylabel(\"Verbrauch\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
